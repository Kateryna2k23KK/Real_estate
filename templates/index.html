<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real Estate Analysis</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='styles/style.css') }}">
</head>
<body>



    <!-- The image that takes up the entire screen -->
    <img src="{{ url_for('static', filename='images/rs1.JPEG') }}" alt="Background Image">



    <div style="text-align: right; margin-right: 30px; margin-top: 20px;">
        <br><p font-size: 18px;">The work was done by Kateryna Yakymenko </p>
        <br><p font-size: 18px;">The data was taken from Kaggle (House Prices - Advanced Regression Techniques)</p>
    </div>



    <!-- Container with content 1 -->
    <div class="content">
        <h1 class="heading">Task:</h1>
        <p class="paragraph" style="margin-bottom: 0;" >
            Develop a system that predicts the price of real estate based on various factors such as house size, number of rooms, property type, and other characteristics. The system should be capable of processing historical data on real estate prices and providing accurate predictions for new properties.
            The goal of the project is to develop a system that predicts real estate prices based on multiple factors, including square footage, number of rooms, location, and other features.
        </p>

        <h1 class="heading">Goal:</h1>
        <p class="paragraph" style="margin-bottom: 0;">
            To accomplish this task, it is necessary to collect and process data using machine learning methods, then train models (Linear Regression, Random Forest, and XGBoost) to accurately predict house prices based on these factors.
            It is also important to evaluate the models' performance using accuracy metrics such as MAE and RMSE to select the most suitable model for real-world conditions.
        </p>
    </div>




    <!-- Importing libraries -->
    <h1 class="heading">Importing libraries</h1>

    <p class="paragraph">
        To begin the analysis, the following libraries need to be imported, as they will be used throughout the process. These libraries provide essential tools for data manipulation, model training, and performance evaluation. The list of libraries:
    <ul class="library-list">
        <li><strong>pandas</strong> - for loading and manipulating data.</li>
        <li><strong>numpy</strong> - for performing numerical operations.</li>
        <li><strong>matplotlib.pyplot and seaborn</strong> - for visualizing data and creating plots.</li>
        <li><strong>sklearn.model_selection</strong> - for splitting the data into training and test sets, cross-validation, and hyperparameter tuning.</li>
        <li><strong>sklearn.preprocessing</strong> - for scaling and normalizing the data.</li>
        <li><strong>sklearn.linear_model</strong> - for training and evaluating the linear regression model.</li>
        <li><strong>sklearn.ensemble</strong> - for working with the random forest model.</li>
        <li><strong>xgboost</strong> - for working with the XGBoost model.</li>
        <li><strong>sklearn.metrics</strong> - for evaluating model performance using metrics like RMSE, RÂ², and MSE.</li>
        <li><strong>shap</strong> - for interpreting model predictions and visualizing feature importance.</li>
        <li><strong>sklearn.decomposition</strong> - for dimensionality reduction using PCA.</li>
    </ul>
    </p>



    <!-- Data loading and initial data analysis -->
    <h1 class="heading">Data loading and initial data analysis</h1>
    <div class="image-container">
        <!-- mage for the second paragraph -->
        <img src="{{ url_for('static', filename='images/1.1.png') }}" alt="Image under Heading 4" class="sub-image-small">
        <img src="{{ url_for('static', filename='images/1.2.png') }}" alt="Image under Heading 4" class="sub-image-small">
    </div>
    <p class="paragraph">
        The data contains 1460 rows and 81 columns. The target variable, `SalePrice`, ranges from 34,900 to 755,000, with an average value of 180,921. There are missing values in some columns, such as `LotFrontage`, `MasVnrType`, `PoolQC`, and others. Columns with numerical data, such as `OverallQual`, `GrLivArea`, and `TotRmsAbvGrd`, have a strong correlation with the sale price.
    </p>

    <p class="paragraph">
       The dataset includes many categorical features, such as `MSZoning`, `Street`, and `GarageType`, which require encoding for use in models. Additionally, numerical data will need to be scaled to improve the performance of machine learning models.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
            # 1. Load the data<br>
            train_data = pd.read_csv('...')<br><br>

            # 2. Initial data analysis<br>
            print(train_data.head())  # First 5 rows<br>
            print(train_data.info())  # Information about columns<br>
            print(train_data.describe())  # Descriptive statistics for numerical data
    </span>
    </p>



    <!-- Handling missing values and encoding categorical variables -->
    <h1 class="heading">Handling missing values and encoding categorical variables</h1>
    <p class="paragraph">
        We filled missing values in numerical columns like `LotFrontage` with the median, and in categorical columns such as `Alley`, `MasVnrType`, with the mode. After that, we removed rows with remaining missing values. Then, we applied One-Hot Encoding to categorical features, transforming them into binary columns. This process helped prepare the data for analysis and modeling, minimizing information loss and correctly handling categorical variables.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
            # 3. Handle missing values<br>
            train_data['LotFrontage'] = train_data['LotFrontage'].fillna(train_data['LotFrontage'].median())<br>
            train_data['Alley'] = train_data['Alley'].fillna(train_data['Alley'].mode()[0])<br>
            train_data['MasVnrType'] = train_data['MasVnrType'].fillna(train_data['MasVnrType'].mode()[0])<br>
            train_data['BsmtQual'] = train_data['BsmtQual'].fillna(train_data['BsmtQual'].mode()[0])<br>
            train_data['BsmtCond'] = train_data['BsmtCond'].fillna(train_data['BsmtCond'].mode()[0])<br>
            train_data['BsmtExposure'] = train_data['BsmtExposure'].fillna(train_data['BsmtExposure'].mode()[0])<br>
            train_data['BsmtFinType1'] = train_data['BsmtFinType1'].fillna(train_data['BsmtFinType1'].mode()[0])<br>
            train_data['BsmtFinType2'] = train_data['BsmtFinType2'].fillna(train_data['BsmtFinType2'].mode()[0])<br>
            train_data['Electrical'] = train_data['Electrical'].fillna(train_data['Electrical'].mode()[0])<br>
            train_data['FireplaceQu'] = train_data['FireplaceQu'].fillna(train_data['FireplaceQu'].mode()[0])<br>
            train_data['GarageType'] = train_data['GarageType'].fillna(train_data['GarageType'].mode()[0])<br>
            train_data['GarageFinish'] = train_data['GarageFinish'].fillna(train_data['GarageFinish'].mode()[0])<br>
            train_data['GarageQual'] = train_data['GarageQual'].fillna(train_data['GarageQual'].mode()[0])<br>
            train_data['GarageCond'] = train_data['GarageCond'].fillna(train_data['GarageCond'].mode()[0])<br>
            train_data['PoolQC'] = train_data['PoolQC'].fillna(train_data['PoolQC'].mode()[0])<br>
            train_data['Fence'] = train_data['Fence'].fillna(train_data['Fence'].mode()[0])<br>
            train_data['MiscFeature'] = train_data['MiscFeature'].fillna(train_data['MiscFeature'].mode()[0])<br><br>

            # Remove rows with remaining missing values<br>
            train_data_cleaned = train_data.dropna()<br><br>

            # 4. One-Hot Encoding of categorical variables<br>
            train_data_cleaned = pd.get_dummies(train_data_cleaned)<br>
    </span>
    </p>



    <!-- Split and Standardize the Data -->
    <h1 class="heading">Split and Standardize the Data</h1>
    <p class="paragraph">
        We split the data into features (`X`) and the target variable (`y`), separating the `SalePrice` column from the rest of the dataset. Then, we standardized the numerical features using `StandardScaler`, which transforms the data to have a mean of 0 and a standard deviation of 1. This step helps improve the performance of machine learning algorithms.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
        # 5. Split the data into features and target variable<br>
        X = train_data_cleaned.drop('SalePrice', axis=1)<br>
        y = train_data_cleaned['SalePrice']<br><br>

        # 6. Standardize the numerical data<br>
        scaler = StandardScaler()<br>
        X_scaled = scaler.fit_transform(X)
    </span>
    </p>



    <!-- Split the Data into Training and Testing Sets -->
    <h1 class="heading">Split the Data into Training and Testing Sets</h1>
    <p class="paragraph">
        The data was split into training and testing sets, with 80% of the data used for training the model and 20% reserved for testing. This approach helps evaluate the model's performance on unseen data and prevents overfitting. The `random_state` value ensures the reproducibility of the data splitting.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
        # 7. Split the data into training and testing sets<br>
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    </span>
    </p>




    <!-- Outlier Analysis -->
    <h1 class="heading">Outlier Analysis</h1>
    <p class="paragraph">
        Outliers were identified using Z-scores, and data points with a Z-score greater than 3 were removed from the training set. This step helped eliminate extreme values that could negatively affect the model, improving its stability and accuracy.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
        # 8. Outlier analysis<br>
        # Use Z-scores to identify outliers<br>
        from scipy.stats import zscore<br><br>

        z_scores = np.abs(zscore(X_train))<br>
        X_train_cleaned = X_train[(z_scores < 3).all(axis=1)]<br>
        y_train_cleaned = y_train[(z_scores < 3).all(axis=1)]
    </span>
    </p>







    <!-- Train Linear Regression Model -->
    <h1 class="heading">Train Linear Regression Model</h1>
    <p class="paragraph">
       The Linear Regression model was trained using the cleaned and preprocessed training data. This step establishes a baseline model to predict house prices based on the features, helping to evaluate the relationship between the predictors and the target variable.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
        # 9. Train Linear Regression model<br>
        lr_model = LinearRegression()<br>
        lr_model.fit(X_train, y_train)
    </span>
    </p>



    <!-- Predictions using Linear Regression -->
    <h1 class="heading">Predictions using Linear Regression</h1>
    <p class="paragraph">
        After training the linear regression model, predictions were made for the test dataset. This allows assessing how well the model can predict house prices based on new data it hasn't seen during training.
    </p>

    <p class="paragraph">
        <strong>Code:</strong> <span style="font-style: italic; font-size: small;">
        # 10. Predict using Linear Regression<br>
        y_pred_lr = lr_model.predict(X_test)
    </span>
    </p>








</body>
</html>
